{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this notebook is to see how easy it is to train an algorithm to find new entities, following the instructions from https://spacy.io/usage/training. For this notebook we will train two entities: vehicle and name.\n",
    "We will use a dataset created by myself which has very little data (Is my intention to with time add more). This data was created by taking random phrases from wikipedia and news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "import warnings\n",
    "\n",
    "#the following function could be easly generalized to allow N entities, but for the time being only allows the two entities mentioned.\n",
    "def create_training_data(path, lower = False, both = False):\n",
    "    train_csv = pd.read_csv(path)\n",
    "\n",
    "    train_csv[\"vehicle_list\"]= train_csv[\"vehicle\"].str.split(\",\")\n",
    "    train_csv[\"vehicle_list\"]= train_csv[\"vehicle_list\"].fillna({i: [] for i in train_csv.index})\n",
    "    train_csv[\"vehicle_list\"] = train_csv[\"vehicle_list\"].apply(lambda x: [y.lstrip() for y in x])\n",
    "    \n",
    "    train_csv[\"name_list\"]= train_csv[\"name\"].str.split(\",\")\n",
    "    train_csv[\"name_list\"]= train_csv[\"name_list\"].fillna({i: [] for i in train_csv.index})\n",
    "    train_csv[\"name_list\"] = train_csv[\"name_list\"].apply(lambda x: [y.lstrip() for y in x])\n",
    "     \n",
    "    all_locs_vehicle = []\n",
    "    Label = \"VEHICLE\"\n",
    "    for i in range(len(train_csv)):\n",
    "        location=[]\n",
    "        for entity in train_csv[\"vehicle_list\"][i]:\n",
    "            ini = train_csv.loc[i,\"text\"].find(entity)\n",
    "            fin = ini + len(entity)\n",
    "            location.append((ini, fin, Label))\n",
    "        all_locs_vehicle.append(location)\n",
    "        \n",
    "    all_locs_name = []\n",
    "    Label = \"NAME\"\n",
    "    for i in range(len(train_csv)):\n",
    "        location=[]\n",
    "        for entity in train_csv[\"name_list\"][i]:\n",
    "            ini = train_csv.loc[i,\"text\"].find(entity)\n",
    "            fin = ini + len(entity)\n",
    "            location.append((ini, fin, Label))\n",
    "        all_locs_name.append(location)\n",
    "         \n",
    "    all_locs = [a + b for a, b in zip(all_locs_vehicle, all_locs_name)]    \n",
    "    \n",
    "    train_csv[\"location\"] = all_locs\n",
    "    train_csv[\"train_dict\"] = train_csv[\"location\"].apply(lambda x: {\"entities\":x})\n",
    "    \n",
    "    #Convert all the text to lowercase\n",
    "    if lower:\n",
    "        train_csv[\"train_data\"] = list(zip(train_csv[\"text\"].str.lower(), train_csv[\"train_dict\"]))\n",
    "    else:\n",
    "        train_csv[\"train_data\"] = list(zip(train_csv[\"text\"], train_csv[\"train_dict\"]))\n",
    "\n",
    "    #Double de data by creating a dataset with the original text plus all the text in lowercase.\n",
    "    if both:\n",
    "        train_csv[\"train_data\"] = list(zip(train_csv[\"text\"].str.lower(), train_csv[\"train_dict\"]))\n",
    "        train_csv[\"train_data_lower\"] = list(zip(train_csv[\"text\"], train_csv[\"train_dict\"]))\n",
    "        train_data = list(train_csv[\"train_data\"]) + list(train_csv[\"train_data_lower\"])\n",
    "    \n",
    "    else:\n",
    "        train_data = list(train_csv[\"train_data\"])\n",
    "\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def train_label(path_data, model=None, lower = False, both = False, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "        \n",
    "    TRAIN_DATA = create_training_data(path_data,\n",
    "                                   lower = lower, both = both)\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the training, let's just look at the data the way it comes in the CSV. It is important that the CSV comes this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At first, Ford in Germany and Ford in Britain ...</td>\n",
       "      <td>Ford Transit, Ford Escort, Ford Capri</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The operation, branded NedCar, began producing...</td>\n",
       "      <td>Mitsubishi Carisma, Volvo S40/V40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Volvo T5 petrol engine was used in the For...</td>\n",
       "      <td>Volvo T5, Ford Focus ST</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In October 2016, Mercedes unveiled the X-Class...</td>\n",
       "      <td>Nissan Navara</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Mercedes-Benz S400 BlueHYBRID was launched...</td>\n",
       "      <td>Mercedes-Benz S400 BlueHYBRID</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  At first, Ford in Germany and Ford in Britain ...   \n",
       "1  The operation, branded NedCar, began producing...   \n",
       "2  The Volvo T5 petrol engine was used in the For...   \n",
       "3  In October 2016, Mercedes unveiled the X-Class...   \n",
       "4  The Mercedes-Benz S400 BlueHYBRID was launched...   \n",
       "\n",
       "                                 vehicle name  \n",
       "0  Ford Transit, Ford Escort, Ford Capri  NaN  \n",
       "1      Mitsubishi Carisma, Volvo S40/V40  NaN  \n",
       "2                Volvo T5, Ford Focus ST  NaN  \n",
       "3                          Nissan Navara  NaN  \n",
       "4          Mercedes-Benz S400 BlueHYBRID  NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"D:/ner/vehicles_ner.csv\")\n",
    "\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Multi award-winning designer Gert-Johan Coetze...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gert-Johan Coetzee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>The deaths of Chadwick Boseman, Kobe Brant and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chadwick Boseman, Kobe Brant, Naya Rivera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>In a late June interview, singer August Alsina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August Alsina, Jada Pinkett Smith, Will Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>The Duchess of Sussex has invested in Clevr Bl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hannah Mendoza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>When Rita Wilson's Golden Globes hair and make...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rita Wilson, Chrissy Teigen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text vehicle  \\\n",
       "63  Multi award-winning designer Gert-Johan Coetze...     NaN   \n",
       "64  The deaths of Chadwick Boseman, Kobe Brant and...     NaN   \n",
       "65  In a late June interview, singer August Alsina...     NaN   \n",
       "66  The Duchess of Sussex has invested in Clevr Bl...     NaN   \n",
       "67  When Rita Wilson's Golden Globes hair and make...     NaN   \n",
       "\n",
       "                                             name  \n",
       "63                             Gert-Johan Coetzee  \n",
       "64      Chadwick Boseman, Kobe Brant, Naya Rivera  \n",
       "65  August Alsina, Jada Pinkett Smith, Will Smith  \n",
       "66                                 Hannah Mendoza  \n",
       "67                    Rita Wilson, Chrissy Teigen  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the CSV is very simple, it requires one column with text, it can be simple and short.\n",
    "In the next columns one needs to add the entity that is in the text, written in EXACTLY the same way. If there is more than one entity, then they should be separated by commas.\n",
    "It is very important than non of the entities is a subset of another. for example \"Will Smith\" and \"Smith\" cannot be in the same row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train 4 algorithms: three of them built on top of an existing model \"en_core_web_sm\", and with the possible combinations of training with capital letters, training only in lowercase, and training doubling the dataset to have each data row both with capitals and in lowercase. The 4 model is built from scratch and with both the lowercase and capital dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_sm'\n",
      "68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wegma\\Anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"For 2021, the Corolla showed out with interior and...\" with entities \"[(14, 22, 'VEHICLE')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1768.6686540842056}\n",
      "Losses {'ner': 1742.1710683107376}\n",
      "Losses {'ner': 1502.2416961193085}\n",
      "Losses {'ner': 1475.2510503530502}\n",
      "Losses {'ner': 1376.4595583677292}\n",
      "Losses {'ner': 1388.804090499878}\n",
      "Losses {'ner': 1368.1998279094696}\n",
      "Losses {'ner': 1322.5616748332977}\n",
      "Losses {'ner': 1318.2733652591705}\n",
      "Losses {'ner': 1283.64140021801}\n",
      "Losses {'ner': 1292.1034083366394}\n",
      "Losses {'ner': 1275.2201805114746}\n",
      "Losses {'ner': 1301.2823269367218}\n",
      "Losses {'ner': 1263.9467059373856}\n",
      "Losses {'ner': 1245.7258214950562}\n",
      "Losses {'ner': 1241.980040192604}\n",
      "Losses {'ner': 1241.4477925300598}\n",
      "Losses {'ner': 1273.932140827179}\n",
      "Losses {'ner': 1275.4104261398315}\n",
      "Losses {'ner': 1226.0862768888474}\n",
      "Losses {'ner': 1208.425077199936}\n",
      "Losses {'ner': 1218.3708946704865}\n",
      "Losses {'ner': 1224.6185513734818}\n",
      "Losses {'ner': 1229.113537788391}\n",
      "Losses {'ner': 1207.2272646427155}\n",
      "Losses {'ner': 1192.838204741478}\n",
      "Losses {'ner': 1231.2462066411972}\n",
      "Losses {'ner': 1191.1409285068512}\n",
      "Losses {'ner': 1196.111690044403}\n",
      "Losses {'ner': 1200.746078491211}\n",
      "Losses {'ner': 1213.551649570465}\n",
      "Losses {'ner': 1215.9473527669907}\n",
      "Losses {'ner': 1222.5953792333603}\n",
      "Losses {'ner': 1193.7637511491776}\n",
      "Losses {'ner': 1173.2285984754562}\n",
      "Losses {'ner': 1186.2444168925285}\n",
      "Losses {'ner': 1193.1755695343018}\n",
      "Losses {'ner': 1196.1499429941177}\n",
      "Losses {'ner': 1195.6843259334564}\n",
      "Losses {'ner': 1188.262011885643}\n",
      "Losses {'ner': 1177.2249032258987}\n",
      "Losses {'ner': 1183.420872092247}\n",
      "Losses {'ner': 1166.9714677333832}\n",
      "Losses {'ner': 1201.9314600229263}\n",
      "Losses {'ner': 1161.5183032751083}\n",
      "Losses {'ner': 1204.1567472219467}\n",
      "Losses {'ner': 1157.691416501999}\n",
      "Losses {'ner': 1179.7841515541077}\n",
      "Losses {'ner': 1145.4521883130074}\n",
      "Losses {'ner': 1166.6723957061768}\n",
      "Losses {'ner': 1190.3832359313965}\n",
      "Losses {'ner': 1158.0915195941925}\n",
      "Losses {'ner': 1132.995331645012}\n",
      "Losses {'ner': 1192.7042130231857}\n",
      "Losses {'ner': 1147.3210273981094}\n",
      "Losses {'ner': 1181.719155073166}\n",
      "Losses {'ner': 1171.790156841278}\n",
      "Losses {'ner': 1158.409185051918}\n",
      "Losses {'ner': 1163.041917681694}\n",
      "Losses {'ner': 1130.0185079574585}\n",
      "Losses {'ner': 1173.4219361543655}\n",
      "Losses {'ner': 1146.966048836708}\n",
      "Losses {'ner': 1175.8984751701355}\n",
      "Losses {'ner': 1160.8281720876694}\n",
      "Losses {'ner': 1161.511958181858}\n",
      "Losses {'ner': 1152.3968632221222}\n",
      "Losses {'ner': 1187.0063569545746}\n",
      "Losses {'ner': 1133.8331370353699}\n",
      "Losses {'ner': 1147.2628657221794}\n",
      "Losses {'ner': 1155.307445883751}\n",
      "Losses {'ner': 1151.3318122774363}\n",
      "Losses {'ner': 1170.4697321653366}\n",
      "Losses {'ner': 1183.4198870658875}\n",
      "Losses {'ner': 1145.7699842453003}\n",
      "Losses {'ner': 1170.1970678567886}\n",
      "Losses {'ner': 1133.493180513382}\n",
      "Losses {'ner': 1178.746127128601}\n",
      "Losses {'ner': 1150.613096356392}\n",
      "Losses {'ner': 1164.547343313694}\n",
      "Losses {'ner': 1149.2508099079132}\n",
      "Losses {'ner': 1150.7123419642448}\n",
      "Losses {'ner': 1157.7824550271034}\n",
      "Losses {'ner': 1140.8425462245941}\n",
      "Losses {'ner': 1169.2921668291092}\n",
      "Losses {'ner': 1137.7692843675613}\n",
      "Losses {'ner': 1139.5390413999557}\n",
      "Losses {'ner': 1172.6140197515488}\n",
      "Losses {'ner': 1153.5081197023392}\n",
      "Losses {'ner': 1116.6654943823814}\n",
      "Losses {'ner': 1129.46240401268}\n",
      "Losses {'ner': 1165.3711148500443}\n",
      "Losses {'ner': 1124.0497163534164}\n",
      "Losses {'ner': 1171.7886667251587}\n",
      "Losses {'ner': 1146.7489560842514}\n",
      "Losses {'ner': 1152.5356007814407}\n",
      "Losses {'ner': 1116.0393956899643}\n",
      "Losses {'ner': 1139.412437915802}\n",
      "Losses {'ner': 1160.7904620170593}\n",
      "Losses {'ner': 1140.3031556606293}\n",
      "Losses {'ner': 1142.6986874043941}\n",
      "Loaded model 'en_core_web_sm'\n",
      "68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wegma\\Anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"for 2021, the corolla showed out with interior and...\" with entities \"[(14, 22, 'VEHICLE')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1790.6087880134583}\n",
      "Losses {'ner': 1714.0673192739487}\n",
      "Losses {'ner': 1571.4035007953644}\n",
      "Losses {'ner': 1383.2840592861176}\n",
      "Losses {'ner': 1376.8488439321518}\n",
      "Losses {'ner': 1354.6153129339218}\n",
      "Losses {'ner': 1316.193055152893}\n",
      "Losses {'ner': 1300.7367210388184}\n",
      "Losses {'ner': 1294.7037624120712}\n",
      "Losses {'ner': 1270.8861352205276}\n",
      "Losses {'ner': 1343.9252434372902}\n",
      "Losses {'ner': 1312.1342871189117}\n",
      "Losses {'ner': 1277.5457113981247}\n",
      "Losses {'ner': 1271.1938561201096}\n",
      "Losses {'ner': 1262.7533793449402}\n",
      "Losses {'ner': 1228.3470081090927}\n",
      "Losses {'ner': 1269.2025427818298}\n",
      "Losses {'ner': 1269.315601348877}\n",
      "Losses {'ner': 1225.7581905126572}\n",
      "Losses {'ner': 1205.9436852931976}\n",
      "Losses {'ner': 1223.5385637879372}\n",
      "Losses {'ner': 1265.9525990486145}\n",
      "Losses {'ner': 1219.2801077365875}\n",
      "Losses {'ner': 1227.6608184576035}\n",
      "Losses {'ner': 1230.6984890699387}\n",
      "Losses {'ner': 1213.0778533220291}\n",
      "Losses {'ner': 1186.899279475212}\n",
      "Losses {'ner': 1210.1419492959976}\n",
      "Losses {'ner': 1209.8850654363632}\n",
      "Losses {'ner': 1205.4022814035416}\n",
      "Losses {'ner': 1202.5573867559433}\n",
      "Losses {'ner': 1216.0791450738907}\n",
      "Losses {'ner': 1251.4951165914536}\n",
      "Losses {'ner': 1199.650382757187}\n",
      "Losses {'ner': 1153.388262271881}\n",
      "Losses {'ner': 1219.0225549936295}\n",
      "Losses {'ner': 1213.3427761793137}\n",
      "Losses {'ner': 1187.5362854003906}\n",
      "Losses {'ner': 1178.8275229930878}\n",
      "Losses {'ner': 1189.2801140546799}\n",
      "Losses {'ner': 1186.6953841745853}\n",
      "Losses {'ner': 1171.541586279869}\n",
      "Losses {'ner': 1190.8330141305923}\n",
      "Losses {'ner': 1189.9513530731201}\n",
      "Losses {'ner': 1187.3770061731339}\n",
      "Losses {'ner': 1185.9051852226257}\n",
      "Losses {'ner': 1171.4043219089508}\n",
      "Losses {'ner': 1186.6584073901176}\n",
      "Losses {'ner': 1167.7188891768456}\n",
      "Losses {'ner': 1163.775687098503}\n",
      "Losses {'ner': 1189.7324131727219}\n",
      "Losses {'ner': 1177.0946009159088}\n",
      "Losses {'ner': 1182.7001118659973}\n",
      "Losses {'ner': 1132.6893128156662}\n",
      "Losses {'ner': 1176.6578074097633}\n",
      "Losses {'ner': 1166.2177602052689}\n",
      "Losses {'ner': 1184.3276640176773}\n",
      "Losses {'ner': 1188.4022711515427}\n",
      "Losses {'ner': 1173.7496349811554}\n",
      "Losses {'ner': 1159.6667749881744}\n",
      "Losses {'ner': 1179.7861802577972}\n",
      "Losses {'ner': 1160.294122338295}\n",
      "Losses {'ner': 1213.0173434019089}\n",
      "Losses {'ner': 1165.2919323444366}\n",
      "Losses {'ner': 1183.6928015947342}\n",
      "Losses {'ner': 1168.0303851366043}\n",
      "Losses {'ner': 1163.1843284964561}\n",
      "Losses {'ner': 1174.9251619577408}\n",
      "Losses {'ner': 1172.6731643676758}\n",
      "Losses {'ner': 1142.7175928354263}\n",
      "Losses {'ner': 1176.5492728352547}\n",
      "Losses {'ner': 1166.247426867485}\n",
      "Losses {'ner': 1164.1262912154198}\n",
      "Losses {'ner': 1180.868444442749}\n",
      "Losses {'ner': 1149.3272641897202}\n",
      "Losses {'ner': 1137.7934365272522}\n",
      "Losses {'ner': 1157.0727435350418}\n",
      "Losses {'ner': 1175.3518512248993}\n",
      "Losses {'ner': 1149.1561864614487}\n",
      "Losses {'ner': 1136.8533091545105}\n",
      "Losses {'ner': 1141.9345970153809}\n",
      "Losses {'ner': 1140.5607265233994}\n",
      "Losses {'ner': 1171.2140597105026}\n",
      "Losses {'ner': 1143.5841495990753}\n",
      "Losses {'ner': 1155.025512933731}\n",
      "Losses {'ner': 1158.9518493413925}\n",
      "Losses {'ner': 1171.4450789690018}\n",
      "Losses {'ner': 1154.7779035568237}\n",
      "Losses {'ner': 1134.4147861003876}\n",
      "Losses {'ner': 1125.529477596283}\n",
      "Losses {'ner': 1126.264972448349}\n",
      "Losses {'ner': 1153.313484430313}\n",
      "Losses {'ner': 1162.2230150699615}\n",
      "Losses {'ner': 1134.5910139083862}\n",
      "Losses {'ner': 1126.8257851600647}\n",
      "Losses {'ner': 1159.962693452835}\n",
      "Losses {'ner': 1150.4601628780365}\n",
      "Losses {'ner': 1135.77656686306}\n",
      "Losses {'ner': 1136.198498249054}\n",
      "Losses {'ner': 1145.527770280838}\n",
      "Loaded model 'en_core_web_sm'\n",
      "136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wegma\\Anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"for 2021, the corolla showed out with interior and...\" with entities \"[(14, 22, 'VEHICLE')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\wegma\\Anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"For 2021, the Corolla showed out with interior and...\" with entities \"[(14, 22, 'VEHICLE')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3464.2047604322433}\n",
      "Losses {'ner': 2917.609674692154}\n",
      "Losses {'ner': 2753.6114585399628}\n",
      "Losses {'ner': 2676.9799662828445}\n",
      "Losses {'ner': 2669.698263168335}\n",
      "Losses {'ner': 2596.9201174378395}\n",
      "Losses {'ner': 2526.5871664881706}\n",
      "Losses {'ner': 2510.0561252832413}\n",
      "Losses {'ner': 2497.633808374405}\n",
      "Losses {'ner': 2530.214335203171}\n",
      "Losses {'ner': 2491.6720004081726}\n",
      "Losses {'ner': 2448.249148964882}\n",
      "Losses {'ner': 2443.9571385383606}\n",
      "Losses {'ner': 2469.6788225769997}\n",
      "Losses {'ner': 2421.3908796310425}\n",
      "Losses {'ner': 2461.7745512723923}\n",
      "Losses {'ner': 2382.7855271697044}\n",
      "Losses {'ner': 2450.437362074852}\n",
      "Losses {'ner': 2436.4655747413635}\n",
      "Losses {'ner': 2386.881347298622}\n",
      "Losses {'ner': 2359.5336021780968}\n",
      "Losses {'ner': 2363.2993045449257}\n",
      "Losses {'ner': 2364.3559769392014}\n",
      "Losses {'ner': 2339.355598807335}\n",
      "Losses {'ner': 2347.350609242916}\n",
      "Losses {'ner': 2342.3463760614395}\n",
      "Losses {'ner': 2387.7337332069874}\n",
      "Losses {'ner': 2334.444981813431}\n",
      "Losses {'ner': 2371.323117852211}\n",
      "Losses {'ner': 2317.732364833355}\n",
      "Losses {'ner': 2355.357651233673}\n",
      "Losses {'ner': 2362.754144668579}\n",
      "Losses {'ner': 2328.122970700264}\n",
      "Losses {'ner': 2361.12766623497}\n",
      "Losses {'ner': 2324.008503675461}\n",
      "Losses {'ner': 2362.8711729049683}\n",
      "Losses {'ner': 2352.892618060112}\n",
      "Losses {'ner': 2410.233139038086}\n",
      "Losses {'ner': 2349.745646595955}\n",
      "Losses {'ner': 2322.2358179092407}\n",
      "Losses {'ner': 2323.4989609718323}\n",
      "Losses {'ner': 2320.386904001236}\n",
      "Losses {'ner': 2341.3925540447235}\n",
      "Losses {'ner': 2343.0502729415894}\n",
      "Losses {'ner': 2308.7995333075523}\n",
      "Losses {'ner': 2286.0262026786804}\n",
      "Losses {'ner': 2277.4940920472145}\n",
      "Losses {'ner': 2346.638655871153}\n",
      "Losses {'ner': 2295.2337017059326}\n",
      "Losses {'ner': 2259.864735722542}\n",
      "Losses {'ner': 2300.620655916631}\n",
      "Losses {'ner': 2265.548713028431}\n",
      "Losses {'ner': 2275.6533106565475}\n",
      "Losses {'ner': 2283.519676923752}\n",
      "Losses {'ner': 2276.3684628009796}\n",
      "Losses {'ner': 2311.121612548828}\n",
      "Losses {'ner': 2321.7620269060135}\n",
      "Losses {'ner': 2248.883650481701}\n",
      "Losses {'ner': 2302.513606905937}\n",
      "Losses {'ner': 2316.8142099380493}\n",
      "Losses {'ner': 2272.933513522148}\n",
      "Losses {'ner': 2332.8685686588287}\n",
      "Losses {'ner': 2233.3491394519806}\n",
      "Losses {'ner': 2278.447558283806}\n",
      "Losses {'ner': 2285.0329493284225}\n",
      "Losses {'ner': 2271.0836973786354}\n",
      "Losses {'ner': 2257.154221713543}\n",
      "Losses {'ner': 2334.3151319026947}\n",
      "Losses {'ner': 2270.8933448195457}\n",
      "Losses {'ner': 2251.641602396965}\n",
      "Losses {'ner': 2279.0561668872833}\n",
      "Losses {'ner': 2248.866276025772}\n",
      "Losses {'ner': 2304.6016432193574}\n",
      "Losses {'ner': 2207.842274606228}\n",
      "Losses {'ner': 2255.0511357188225}\n",
      "Losses {'ner': 2298.034805059433}\n",
      "Losses {'ner': 2280.9077462516725}\n",
      "Losses {'ner': 2306.8933407068253}\n",
      "Losses {'ner': 2251.006829559803}\n",
      "Losses {'ner': 2274.717091321945}\n",
      "Losses {'ner': 2256.89563703537}\n",
      "Losses {'ner': 2278.5404785871506}\n",
      "Losses {'ner': 2242.6938629746437}\n",
      "Losses {'ner': 2294.212401866913}\n",
      "Losses {'ner': 2245.5549258589745}\n",
      "Losses {'ner': 2266.1892528533936}\n",
      "Losses {'ner': 2262.353677392006}\n",
      "Losses {'ner': 2266.25361597538}\n",
      "Losses {'ner': 2245.3715773820877}\n",
      "Losses {'ner': 2230.4115582704544}\n",
      "Losses {'ner': 2230.3297842741013}\n",
      "Losses {'ner': 2282.7459884285927}\n",
      "Losses {'ner': 2259.610123872757}\n",
      "Losses {'ner': 2218.297038912773}\n",
      "Losses {'ner': 2269.975783765316}\n",
      "Losses {'ner': 2236.0930656194687}\n",
      "Losses {'ner': 2257.137082695961}\n",
      "Losses {'ner': 2210.6332263946533}\n",
      "Losses {'ner': 2265.0047565102577}\n",
      "Losses {'ner': 2220.1201907992363}\n",
      "Created blank 'en' model\n",
      "136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wegma\\Anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"For 2021, the Corolla showed out with interior and...\" with entities \"[(14, 22, 'VEHICLE')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\wegma\\Anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"for 2021, the corolla showed out with interior and...\" with entities \"[(14, 22, 'VEHICLE')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 949.9281453204676}\n",
      "Losses {'ner': 436.29655079956865}\n",
      "Losses {'ner': 420.23687518074075}\n",
      "Losses {'ner': 444.01948833395363}\n",
      "Losses {'ner': 395.4580296885058}\n",
      "Losses {'ner': 507.65424433472435}\n",
      "Losses {'ner': 368.9197546825548}\n",
      "Losses {'ner': 537.7458909839859}\n",
      "Losses {'ner': 268.7701371210194}\n",
      "Losses {'ner': 404.74303240867687}\n",
      "Losses {'ner': 252.86074340940758}\n",
      "Losses {'ner': 231.02604607614012}\n",
      "Losses {'ner': 228.08295443648436}\n",
      "Losses {'ner': 200.91086739660972}\n",
      "Losses {'ner': 148.48556680731565}\n",
      "Losses {'ner': 146.95419746052318}\n",
      "Losses {'ner': 139.9613335808985}\n",
      "Losses {'ner': 91.41838542402083}\n",
      "Losses {'ner': 83.45099807350645}\n",
      "Losses {'ner': 93.20085436688632}\n",
      "Losses {'ner': 67.74017521927635}\n",
      "Losses {'ner': 66.69221076459806}\n",
      "Losses {'ner': 62.254582596098125}\n",
      "Losses {'ner': 82.1155483535566}\n",
      "Losses {'ner': 46.84873331458119}\n",
      "Losses {'ner': 39.354900051496806}\n",
      "Losses {'ner': 55.21637533534718}\n",
      "Losses {'ner': 37.42706473394224}\n",
      "Losses {'ner': 27.717711873082706}\n",
      "Losses {'ner': 25.97304982353663}\n",
      "Losses {'ner': 28.14714137921314}\n",
      "Losses {'ner': 21.021100232934387}\n",
      "Losses {'ner': 16.818607330751526}\n",
      "Losses {'ner': 37.18137239060316}\n",
      "Losses {'ner': 41.803929770475555}\n",
      "Losses {'ner': 18.904944466700336}\n",
      "Losses {'ner': 25.595358576194293}\n",
      "Losses {'ner': 15.472018102401446}\n",
      "Losses {'ner': 3.6796246876777547}\n",
      "Losses {'ner': 13.0911683586915}\n",
      "Losses {'ner': 8.250609743242437}\n",
      "Losses {'ner': 30.209232537337456}\n",
      "Losses {'ner': 10.147378502875275}\n",
      "Losses {'ner': 8.156142166643622}\n",
      "Losses {'ner': 10.55612368553405}\n",
      "Losses {'ner': 24.526775399324162}\n",
      "Losses {'ner': 11.612607063985486}\n",
      "Losses {'ner': 5.421594455134302}\n",
      "Losses {'ner': 5.306088454597397}\n",
      "Losses {'ner': 3.121006581424781}\n",
      "Losses {'ner': 7.259305079964819}\n",
      "Losses {'ner': 10.897542634881164}\n",
      "Losses {'ner': 8.1546223094368}\n",
      "Losses {'ner': 16.399302444619426}\n",
      "Losses {'ner': 5.045404238889812}\n",
      "Losses {'ner': 7.571558151495627}\n",
      "Losses {'ner': 7.926592339748502}\n",
      "Losses {'ner': 5.673990298012415}\n",
      "Losses {'ner': 5.985297680878577}\n",
      "Losses {'ner': 10.601278658481538}\n",
      "Losses {'ner': 12.852546551583133}\n",
      "Losses {'ner': 0.1329854997548771}\n",
      "Losses {'ner': 8.262184558561692}\n",
      "Losses {'ner': 9.841313156585901}\n",
      "Losses {'ner': 9.90051990942543}\n",
      "Losses {'ner': 9.784602576009705}\n",
      "Losses {'ner': 8.795500809022741}\n",
      "Losses {'ner': 9.144581864506081}\n",
      "Losses {'ner': 1.8043320044885351}\n",
      "Losses {'ner': 10.403084554188567}\n",
      "Losses {'ner': 6.09022659088123}\n",
      "Losses {'ner': 0.0006523411860175542}\n",
      "Losses {'ner': 3.129587995902629}\n",
      "Losses {'ner': 12.247496675509652}\n",
      "Losses {'ner': 13.418146418307506}\n",
      "Losses {'ner': 14.890478791096818}\n",
      "Losses {'ner': 18.797052917182587}\n",
      "Losses {'ner': 9.38913689398595}\n",
      "Losses {'ner': 12.410220452393117}\n",
      "Losses {'ner': 6.2419537041453435}\n",
      "Losses {'ner': 1.103643524092151}\n",
      "Losses {'ner': 8.298918258349769}\n",
      "Losses {'ner': 9.044567337688546}\n",
      "Losses {'ner': 6.613165454711792}\n",
      "Losses {'ner': 21.913936245410625}\n",
      "Losses {'ner': 9.595623552526284}\n",
      "Losses {'ner': 0.22447282722425782}\n",
      "Losses {'ner': 10.144093408098138}\n",
      "Losses {'ner': 0.17072080415503543}\n",
      "Losses {'ner': 1.635207918370765}\n",
      "Losses {'ner': 6.797323538571133}\n",
      "Losses {'ner': 11.57966086800406}\n",
      "Losses {'ner': 10.343328348110127}\n",
      "Losses {'ner': 2.5948829658577264}\n",
      "Losses {'ner': 0.23744450433034126}\n",
      "Losses {'ner': 17.96961095827475}\n",
      "Losses {'ner': 3.9898553885224715}\n",
      "Losses {'ner': 7.901581703412341}\n",
      "Losses {'ner': 11.923764165940643}\n",
      "Losses {'ner': 10.65961939157978}\n"
     ]
    }
   ],
   "source": [
    "nlp1 = train_label(path_data=\"D:/ner/vehicles_ner.csv\", \n",
    "                    model=\"en_core_web_sm\", \n",
    "                    lower = False, both = False,\n",
    "                    n_iter=100)  \n",
    "\n",
    "nlp2 = train_label(path_data=\"D:/ner/vehicles_ner.csv\", \n",
    "                    model=\"en_core_web_sm\", \n",
    "                    lower = True, both = False,\n",
    "                    n_iter=100)  \n",
    "\n",
    "nlp3 = train_label(path_data=\"D:/ner/vehicles_ner.csv\", \n",
    "                    model=\"en_core_web_sm\", \n",
    "                    lower = True, both = True,\n",
    "                    n_iter=100)  \n",
    "\n",
    "nlp4 = train_label(path_data=\"D:/ner/vehicles_ner.csv\", \n",
    "                    model=None, \n",
    "                    lower = True, both = True,\n",
    "                    n_iter=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [\"I was driving a Mercedes-Benz P53\",\n",
    "             \"I was driving a mercedes-menz P53\",\n",
    "             \"I was driving a Aston-Martin P53\",\n",
    "             \"I was driving a Aston Martin P53\",\n",
    "             \"I was driving a Corolla Pizza\",\n",
    "             \"I was eating a Dominos Pizza\",\n",
    "             \"I was eating a Pizza\",\n",
    "             \"DanielÂ´s Pizza is the best\",\n",
    "             \"My name is Daniel Wegman I was born in Mexico City\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Mercedes-Benz P53', 'VEHICLE')]\n",
      "Entities []\n",
      "Entities [('Aston-Martin P53', 'VEHICLE')]\n",
      "Entities [('Aston Martin', 'VEHICLE')]\n",
      "Entities [('Corolla Pizza', 'VEHICLE')]\n",
      "Entities [('Dominos Pizza', 'VEHICLE')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Daniel Wegman', 'NAME')]\n"
     ]
    }
   ],
   "source": [
    "for doc in test_docs:\n",
    "    doc = nlp1(doc)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Mercedes-', 'VEHICLE')]\n",
      "Entities [('mercedes-', 'VEHICLE')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Corolla Pizza', 'VEHICLE')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Daniel Wegman', 'NAME')]\n"
     ]
    }
   ],
   "source": [
    "for doc in test_docs:\n",
    "    doc = nlp2(doc)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Mercedes-Benz P53', 'VEHICLE')]\n",
      "Entities [('mercedes-menz P53', 'VEHICLE')]\n",
      "Entities [('Martin', 'VEHICLE')]\n",
      "Entities []\n",
      "Entities [('Corolla', 'VEHICLE')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Daniel Wegman', 'NAME')]\n"
     ]
    }
   ],
   "source": [
    "for doc in test_docs:\n",
    "    doc = nlp3(doc)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Mercedes-Benz', 'VEHICLE')]\n",
      "Entities [('mercedes-menz', 'VEHICLE')]\n",
      "Entities []\n",
      "Entities [('Martin', 'VEHICLE')]\n",
      "Entities [('Corolla Pizza', 'VEHICLE')]\n",
      "Entities [('Pizza', 'VEHICLE')]\n",
      "Entities [('Pizza', 'VEHICLE')]\n",
      "Entities [('DanielÂ´s Pizza', 'NAME')]\n",
      "Entities [('City', 'VEHICLE')]\n"
     ]
    }
   ],
   "source": [
    "for doc in test_docs:\n",
    "    doc = nlp4(doc)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very clear that since at this point there is not enough data, the training was not completely successful, but it can be seen how the algorithm already started to learn even with this small amount of information. Also it is a better idea to train with both capitalizacion and all in lowercase so the algorithm does not learn just to get entities because they have a capital letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
